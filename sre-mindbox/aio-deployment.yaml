apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-deployment
  labels:
    app: webapp
spec:
  # Минимум 3 реплики необходимо. Учитываем, что кластер мультизональный, поэтому минимально необходимо хотя бы по 1 поду на каждую зону доступности.
  replicas: 3
  selector:
    matchLabels:
      app: webapp
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1        
  template:
    metadata:
      labels:
        app: webapp
    spec:
      affinity:
        podAntiAffinity:
          # Кажется, что лучше использовать preferred правила, чтобы полностью не блокировать деплой, если, к примеру, не хватит нод или зон.
          preferredDuringSchedulingIgnoredDuringExecution:
          # Размазываем поды по зонам
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - webapp
              topologyKey: "topology.kubernetes.io/zone"
          # Размазываем поды по нодам
          - weight: 50
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - webapp
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: web-server
        image: nginx:latest
        ports:
        - name: http
          containerPort: 80
        resources:
          requests:
            cpu: "200m"    # 0.2 ядра
            memory: "128Mi"
          limits:
            cpu: "500m"    # 0.5 ядра
            memory: "256Mi"
        # Ливнесс и рединесс пробы для подов, чтобы можно было следить за состоянием подов и решить задачу с временем инициализации в 10 секунд
        livenessProbe:
          httpGet:
            path: / # Проверка, что приложение "живо"
            port: http
          initialDelaySeconds: 10 # Начать проверку через 10 сек после старта
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: / # Проверка, что приложение готово принимать трафик
            port: http
          initialDelaySeconds: 10
          periodSeconds: 10
      # Время на корректное завершение работы пода (например, закончить обработку запросов)
      terminationGracePeriodSeconds: 30

---
# PDB решает задачу отказоустойчивости, обеспечивая необходимый минимум рабочих подов.
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: webapp-pdb
spec:
  maxUnavailable: 1
  minAvailable: 3
  selector:
    matchLabels:
      app: webapp

---
# HPA решает задачу неравномерной нагрузки, минимального потребления ресурсов и отказоустойчивости
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp-deployment
  minReplicas: 3      
  maxReplicas: 9     # Выбор значения обусловлен тем, что существует вероятность превышения ожидаемой пиковой нагрузки.
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50 # 50% от запрошенных CPU — 0,1 ядра. Это решает задачу "тяжёлых" первых запросов и учитывает дальнейшее потребление.
  behavior:
    scaleDown:
    # защищаемся от флаппинга
      stabilizationWindowSeconds: 300
      policies:
    # не удаляем больше 2 подов в минуту  
      - type: Pods
        value: 2
        periodSeconds: 60

---
# В ТЗ не сказано, где именно располагается кластер. Предполагаю, что это будет облачное решение, поэтому реализуем сервис типа LoadBalancer
# Это решение поможет распределять трафик по всем сервисам с меткой webapp
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: LoadBalancer
  selector:
    app: webapp
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
